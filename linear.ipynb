{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from utils import *\n",
    "from data import *\n",
    "from metrics import *\n",
    "from trainer import *\n",
    "from models import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_architecture_name(hidden_sizes):\n",
    "    \"\"\"Generate a readable name for the architecture.\"\"\"\n",
    "    if not hidden_sizes:\n",
    "        return \"linear\"\n",
    "    return \"->\".join(['input'] + [str(s) for s in hidden_sizes] + ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model: nn.Module, config: Any) -> torch.optim.Optimizer:\n",
    "    \"\"\"Create optimizer with proper parameter groups.\"\"\"\n",
    "    norm_params = []\n",
    "    other_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'norm' in name:\n",
    "            norm_params.append(param)\n",
    "        else:\n",
    "            other_params.append(param)\n",
    "    if config.optimizer == 'adam':\n",
    "        return torch.optim.Adam([\n",
    "            {'params': norm_params, 'weight_decay': 0},\n",
    "            {'params': other_params, 'weight_decay': config.weight_decay}\n",
    "        ], lr=config.learning_rate)\n",
    "    elif config.optimizer == 'adamw':\n",
    "        return torch.optim.AdamW([\n",
    "            {'params': norm_params, 'weight_decay': 0},\n",
    "            {'params': other_params, 'weight_decay': config.weight_decay}\n",
    "        ], lr=config.learning_rate)\n",
    "    else:\n",
    "        return torch.optim.SGD([\n",
    "            {'params': norm_params, 'weight_decay': 0},\n",
    "            {'params': other_params, 'weight_decay': config.weight_decay}\n",
    "        ], lr=config.learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    \"\"\"Run a single experiment with wandb config.\"\"\"\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        \n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        encoder = BinaryEncoder(bits=config.num_bits)\n",
    "        \n",
    "        arch_str = '->'.join(['input'] + [str(s) for s in config.hidden_sizes] + ['output'])\n",
    "        run.name = f\"b{config.num_bits}_{arch_str}_{config.optimizer}_lr{config.learning_rate}\"\n",
    "        if config.use_residual:\n",
    "            run.name += \"_res\"\n",
    "        \n",
    "        # Training data setup\n",
    "        if config.sparse_sampling and config.train_range_max > 1000:\n",
    "            train_nums = (\n",
    "                list(range(1, 101)) +  # Dense for small numbers\n",
    "                list(range(101, 1001, 20)) +  # Sparse for medium\n",
    "                list(range(1001, config.train_range_max, 200))  # Very sparse for large\n",
    "            )\n",
    "            train_data = FizzBuzzDataset(train_nums, encoder)\n",
    "        else:\n",
    "            train_data = FizzBuzzDataset((1, config.train_range_max), encoder)\n",
    "        \n",
    "        # Validation setup\n",
    "        val_range = (config.train_range_max, config.train_range_max + 50)\n",
    "        val_data = FizzBuzzDataset(val_range, encoder)\n",
    "        \n",
    "        # DataLoader setup\n",
    "        train_loader = DataLoader(\n",
    "            train_data, \n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=device=='cuda'\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_data, \n",
    "            batch_size=config.batch_size * 2,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Model setup\n",
    "        model = LinearFizzBuzz(\n",
    "            input_size=encoder.get_input_size(),\n",
    "            hidden_sizes=config.hidden_sizes,\n",
    "            dropout=config.dropout,\n",
    "            activation=config.activation,\n",
    "            use_residual=config.use_residual,\n",
    "            use_layer_norm=config.use_layer_norm,\n",
    "            bottleneck_factor=config.bottleneck_factor,\n",
    "            num_residual_blocks=config.num_residual_blocks\n",
    "        )\n",
    "        \n",
    "        # Training setup\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        optimizer = get_optimizer(model, config)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        if config.lr_scheduler == 'reduce_on_plateau':\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='max',\n",
    "                factor=config.lr_factor,\n",
    "                patience=config.lr_patience,\n",
    "                min_lr=1e-6\n",
    "            )\n",
    "        elif config.lr_scheduler == 'cosine':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=config.epochs,\n",
    "                eta_min=1e-6\n",
    "            )\n",
    "        else:\n",
    "            scheduler = None\n",
    "        \n",
    "        # Training\n",
    "        trainer = FizzBuzzTrainer(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            device=device,\n",
    "            gradient_clip=config.gradient_clip,\n",
    "            use_wandb=True\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        metrics = trainer.train(\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=config.epochs\n",
    "        )\n",
    "        \n",
    "        # Test ranges - reduced sizes\n",
    "        test_ranges = [\n",
    "            (1, 100),       # Training range sample\n",
    "            (101, 500),     # Near generalization\n",
    "            (501, 1000),    # Mid generalization\n",
    "            (1001, 2000)    # Far generalization\n",
    "        ]\n",
    "        \n",
    "        # Test on all ranges\n",
    "        test_results = {}\n",
    "        for start, end in test_ranges:\n",
    "            results = trainer.test(start, end, encoder)\n",
    "            accuracy = evaluate_model_accuracy(results, start, end)\n",
    "            test_results[f\"accuracy_{start}_{end}\"] = accuracy\n",
    "        \n",
    "        # Logging\n",
    "        complexity_stats = model.get_complexity_stats()\n",
    "        wandb.run.summary.update({\n",
    "            **complexity_stats,\n",
    "            'num_bits': config.num_bits,\n",
    "            \"architecture\": arch_str,\n",
    "            \"n_layers\": len(config.hidden_sizes),\n",
    "            \"widest_layer\": max(config.hidden_sizes) if config.hidden_sizes else 4,\n",
    "            \"activation\": config.activation,\n",
    "            \"dropout\": config.dropout,\n",
    "            \"weight_decay\": config.weight_decay,\n",
    "            \"train_range\": config.train_range_max,\n",
    "            \"use_residual\": config.use_residual,\n",
    "            \"use_layer_norm\": config.use_layer_norm,\n",
    "            \"bottleneck_factor\": config.bottleneck_factor,\n",
    "            \"num_residual_blocks\": config.num_residual_blocks,\n",
    "            \"sparse_sampling\": config.sparse_sampling,\n",
    "            **test_results\n",
    "        })\n",
    "        \n",
    "        # Save best model\n",
    "        if test_results[\"accuracy_101_500\"] > wandb.run.summary.get(\"best_accuracy_101_500\", 0):\n",
    "            model_path = os.path.join(wandb.run.dir, \"best_model.pt\")\n",
    "            \n",
    "            # Convert config to a dictionary of basic types\n",
    "            config_dict = {\n",
    "                'num_bits': config.num_bits,\n",
    "                'hidden_sizes': config.hidden_sizes,\n",
    "                'dropout': config.dropout,\n",
    "                'activation': config.activation,\n",
    "                'use_residual': config.use_residual,\n",
    "                'use_layer_norm': config.use_layer_norm,\n",
    "                'bottleneck_factor': config.bottleneck_factor,\n",
    "                'num_residual_blocks': config.num_residual_blocks,\n",
    "                'learning_rate': config.learning_rate,\n",
    "                'weight_decay': config.weight_decay,\n",
    "                'optimizer': config.optimizer,\n",
    "                'train_range_max': config.train_range_max,\n",
    "                'sparse_sampling': config.sparse_sampling\n",
    "            }\n",
    "            \n",
    "            save_dict = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config_dict,\n",
    "                'test_results': test_results,\n",
    "                'complexity_stats': complexity_stats\n",
    "            }\n",
    "            \n",
    "            torch.save(save_dict, model_path)\n",
    "            wandb.save(model_path)\n",
    "            wandb.run.summary[\"best_accuracy_101_500\"] = test_results[\"accuracy_101_500\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 20,\n",
    "        'eta': 2,\n",
    "        'max_iter': 100\n",
    "    },\n",
    "    'parameters': {\n",
    "        # Input representation\n",
    "        'num_bits': {\n",
    "            'values': [16, 24, 32]\n",
    "        },\n",
    "        \n",
    "        # Architecture\n",
    "        'hidden_sizes': {\n",
    "            'values': [\n",
    "                # Medium\n",
    "                [256],\n",
    "                [256, 256],\n",
    "                [256, 256, 256],\n",
    "                [512, 256],\n",
    "                \n",
    "                # Large\n",
    "                [512, 512],\n",
    "                [512, 512, 256],\n",
    "                [256, 256, 256, 256],\n",
    "                \n",
    "                # Bottleneck architectures\n",
    "                [512, 128, 512],\n",
    "                [256, 64, 256],\n",
    "                \n",
    "                # Residual-friendly\n",
    "                [256, 256, 256, 256],\n",
    "                [512, 512, 512]\n",
    "            ]\n",
    "        },\n",
    "        \n",
    "        # Residual configuration\n",
    "        'use_residual': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'use_layer_norm': {\n",
    "            'values': [True, False]\n",
    "        },\n",
    "        'num_residual_blocks': {\n",
    "            'values': [2, 3]\n",
    "        },\n",
    "        'bottleneck_factor': {\n",
    "            'values': [0.25, 0.5]\n",
    "        },\n",
    "        \n",
    "        # Training parameters\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-4,\n",
    "            'max': 5e-3\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'values': [32, 64, 128]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [150, 300, 500]\n",
    "        },\n",
    "        \n",
    "        # Regularization\n",
    "        'dropout': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0,\n",
    "            'max': 0.3\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-6,\n",
    "            'max': 1e-4\n",
    "        },\n",
    "        'gradient_clip': {\n",
    "            'values': [0.0, 1.0, 5.0]\n",
    "        },\n",
    "        \n",
    "        # Optimization\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'adamw']\n",
    "        },\n",
    "        'lr_scheduler': {\n",
    "            'values': ['reduce_on_plateau', 'cosine', 'none']\n",
    "        },\n",
    "        'lr_patience': {\n",
    "            'values': [5, 10, 15]\n",
    "        },\n",
    "        'lr_factor': {\n",
    "            'values': [0.1, 0.5]\n",
    "        },\n",
    "        \n",
    "        # Model components\n",
    "        'activation': {\n",
    "            'values': ['relu', 'gelu']\n",
    "        },\n",
    "        \n",
    "        # Training data configuration\n",
    "        'train_range_max': {\n",
    "            'values': [100, 1000, 10000]\n",
    "        },\n",
    "        'sparse_sampling': {\n",
    "            'values': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'run_cap': 100  # Maximum number of runs in the sweep\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    wandb.login()\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"fizzbuzz-linear\")\n",
    "    wandb.agent(sweep_id, function=run_experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
